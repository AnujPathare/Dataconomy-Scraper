{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install selenium\n",
    "!pip install requests\n",
    "!pip install newspaper3k\n",
    "!pip install beautifulsoup4\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "# Set up Chrome options for running in headless mode\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "# Create a Chrome WebDriver instance\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()  # Maximize the window for better visibility\n",
    "driver.get(\"https://dataconomy.com/\")  # Open the main website\n",
    "\n",
    "# Dictionary to store blog URLs categorized by their type\n",
    "blogs_URL = {}\n",
    "# Dictionary to store blog titles categorized by their type\n",
    "blogs_title_set = {}\n",
    "# Dictionary to store all blog titles categorized by their type\n",
    "blogs_title = {}\n",
    "\n",
    "# Function to navigate to a specific category in the website\n",
    "def go_to_category(driver, category_URL):\n",
    "    driver.get(category_URL)\n",
    "\n",
    "# List of subcategories to scrape blogs from\n",
    "category_sub_URLs = [\"news\", \"topics/data-science/artificial-intelligence\", \"topics/data-science/big-data\", \"topics/data-science/machine-learning\", \"tech-trends/blockchain-tech-trends\", \"cybersecurity\", \"topics/fintech\", \"gaming\", \"topics/internet-of-things\", \"startups\", \"industry/energy-environment\", \"industry/finance\", \"industry/healthcare\", \"industry/industrial-goods-and-services\", \"industry/marketing-sales\", \"industry/retail-and-consumer-industry\", \"industry/technology-and-it-industry\", \"industry/transportation-and-logistics\"]\n",
    "\n",
    "# Function to scrape blogs from the specified subcategories\n",
    "def scrape_blogs(driver, category_sub_URLs, blogs_URL, blogs_title, blogs_title_set):\n",
    "    for category_sub_URL in category_sub_URLs:\n",
    "        go_to_category(driver, f\"https://dataconomy.com/category/{category_sub_URL}/\")\n",
    "\n",
    "        # Scrape the first four blogs on the page\n",
    "        for i in range(1, 5):\n",
    "            blog = driver.find_element(By.CLASS_NAME, f'jeg_hero_item_{i}')\n",
    "            blog_category = blog.find_element(By.CLASS_NAME, 'jeg_post_category').text\n",
    "            blog_title_tag = blog.find_element(By.CLASS_NAME, 'jeg_post_title')\n",
    "\n",
    "            # Initialize dictionaries if not present\n",
    "            if blog_category not in blogs_title:\n",
    "                blogs_title[blog_category] = []\n",
    "                blogs_URL[blog_category] = []\n",
    "                blogs_title_set[blog_category] = set()\n",
    "\n",
    "            # Add unique blog titles and URLs to dictionaries\n",
    "            if blog_title_tag.text not in blogs_title_set[blog_category]:\n",
    "                blogs_title_set[blog_category].add(blog_title_tag.text)\n",
    "                blogs_title[blog_category].append(blog_title_tag.text)\n",
    "                blogs_URL[blog_category].append(blog_title_tag.find_element(By.TAG_NAME, 'a').get_attribute(\"href\"))\n",
    "\n",
    "        page_no = 1\n",
    "\n",
    "        # Continue scraping remaining blogs using pagination\n",
    "        while True:\n",
    "            try:\n",
    "                remaining_blogs = driver.find_elements(By.CLASS_NAME, 'jeg_pl_md_1')\n",
    "\n",
    "                for blog in remaining_blogs:\n",
    "                    blog_category = blog.find_element(By.CLASS_NAME, 'jeg_post_category').text\n",
    "                    blog_title_tag = blog.find_element(By.CLASS_NAME, 'jeg_post_title')\n",
    "\n",
    "                    # Initialize dictionaries if not present\n",
    "                    if blog_category not in blogs_title:\n",
    "                        blogs_title[blog_category] = []\n",
    "                        blogs_URL[blog_category] = []\n",
    "                        blogs_title_set[blog_category] = set()\n",
    "\n",
    "                    # Add unique blog titles and URLs to dictionaries\n",
    "                    if blog_title_tag.text not in blogs_title_set[blog_category]:\n",
    "                        blogs_title_set[blog_category].add(blog_title_tag.text)\n",
    "                        blogs_title[blog_category].append(blog_title_tag.text)\n",
    "                        blogs_URL[blog_category].append(blog_title_tag.find_element(By.TAG_NAME, 'a').get_attribute(\"href\"))\n",
    "\n",
    "                print(f\"Page {page_no} over...\")\n",
    "                page_no += 1\n",
    "\n",
    "                # Scroll to the end of the page to load more blogs\n",
    "                webdriver.ActionChains(driver).send_keys(Keys.END).perform()\n",
    "\n",
    "                # Find and click the next page button\n",
    "                next_page_button = driver.find_element(By.XPATH, \"//a[@class='page_nav next']\")\n",
    "                webdriver.ActionChains(driver).move_to_element(next_page_button).click(next_page_button).perform()\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(f\"All Blogs from https://dataconomy.com/category/{category_sub_URL}/ Registered!\\n\")\n",
    "                break\n",
    "\n",
    "# Call the function to scrape blogs\n",
    "scrape_blogs(driver, category_sub_URLs, blogs_URL, blogs_title, blogs_title_set)\n",
    "\n",
    "# Print the index, category, and URL of each scraped blog\n",
    "index = 1\n",
    "for URL_category, urls in blogs_URL.items():\n",
    "    for url in urls:\n",
    "        print(index, URL_category, url)\n",
    "        index += 1\n",
    "\n",
    "# Function to save blog information to a text file\n",
    "def save_to_text_file(title, link, author_name, author_URL, date, text, summary, folder_name):\n",
    "    # Remove invalid characters from the title to create a valid filename for the text file\n",
    "    filename = os.path.join(folder_name, re.sub(r'[\\/:*?\"<>|]', '', title) + '.txt')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        # Write the information to the file\n",
    "        file.write(f'Title: {title}\\n\\n')\n",
    "        file.write(f'Article Link: {link}\\n\\n')\n",
    "        file.write(f'Author: {author_name}\\n\\n')\n",
    "        file.write(f'Publish Date: {date}\\n\\n')\n",
    "        file.write(f'Article:\\n{text}\\n\\n')\n",
    "        file.write(f'Summary: {summary}\\n\\n')\n",
    "        file.write(f'Other Blogs by {author_name}: {author_URL}\\n')\n",
    "\n",
    "    print(f'Information saved to {filename}')\n",
    "\n",
    "# Iterate through each category and blog URL to scrape and save blog information\n",
    "for URL_category, url_list in blogs_URL.items():\n",
    "    for url in url_list:\n",
    "        time.sleep(5)  # Add a delay to avoid making too many requests in a short time\n",
    "\n",
    "        # Make a request to the URL\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find author information\n",
    "        author = soup.find('div', class_='jeg_meta_author').find('a')\n",
    "\n",
    "        # Create an Article object\n",
    "        article = Article(url)\n",
    "        article.set_html(html)\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "\n",
    "        # Extract article title\n",
    "        title = article.title\n",
    "\n",
    "        filename = os.path.join(URL_category, re.sub(r'[\\/:*?\"<>|]', '', title) + '.txt')\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(filename):\n",
    "            print(f'File {filename} already exists. Skipping...')\n",
    "            continue\n",
    "\n",
    "        # Extract information from the article\n",
    "        link = article.url\n",
    "        author_name = author.text\n",
    "        author_URL = author.get('href')\n",
    "        date = article.publish_date.date()\n",
    "        text = article.text\n",
    "        summary = article.summary\n",
    "\n",
    "        # Save information to a text file in the respective folder\n",
    "        save_to_text_file(title, link, author_name, author_URL, date, text, summary, URL_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install requests\n",
    "# !pip install newspaper3k\n",
    "# !pip install beautifulsoup4\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import requests\n",
    "# from newspaper import Article\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.common.exceptions import NoSuchElementException\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# import time\n",
    "\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless=new\")\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# driver.maximize_window()\n",
    "# driver.get(\"https://dataconomy.com/\")\n",
    "\n",
    "# blogs_URL = {}\n",
    "# blogs_title_set = {}\n",
    "# blogs_title = {}\n",
    "\n",
    "# def go_to_category(driver, category_URL):\n",
    "    \n",
    "#     driver.get(category_URL)\n",
    "\n",
    "# category_sub_URLs = [\"news\", \"topics/data-science/artificial-intelligence\", \"topics/data-science/big-data\", \"topics/data-science/machine-learning\", \"tech-trends/blockchain-tech-trends\", \"cybersecurity\", \"topics/fintech\", \"gaming\", \"topics/internet-of-things\", \"startups\", \"industry/energy-environment\", \"industry/finance\", \"industry/healthcare\", \"industry/industrial-goods-and-services\", \"industry/marketing-sales\", \"industry/retail-and-consumer-industry\", \"industry/technology-and-it-industry\", \"industry/transportation-and-logistics\"]\n",
    "\n",
    "# def scrape_blogs(driver, category_sub_URLs, blogs_URL, blogs_title, blogs_title_set):\n",
    "    \n",
    "#     # first four blogs\n",
    "#     for category_sub_URL in category_sub_URLs:\n",
    "#         go_to_category(driver, f\"https://dataconomy.com/category/{category_sub_URL}/\")\n",
    "\n",
    "#         for i in range(1,5):\n",
    "\n",
    "#             blog = driver.find_element(By.CLASS_NAME, f'jeg_hero_item_{i}')\n",
    "\n",
    "#             blog_category = blog.find_element(By.CLASS_NAME, 'jeg_post_category').text\n",
    "#             blog_title_tag = blog.find_element(By.CLASS_NAME, 'jeg_post_title')\n",
    "\n",
    "#             if blog_category not in blogs_title:\n",
    "#                 blogs_title[blog_category] = []\n",
    "#                 blogs_URL[blog_category] = []\n",
    "#                 blogs_title_set[blog_category] = set()\n",
    "\n",
    "#             if blog_title_tag.text not in blogs_title_set[blog_category]:\n",
    "\n",
    "#                 blogs_title_set[blog_category].add(blog_title_tag.text)\n",
    "#                 blogs_title[blog_category].append(blog_title_tag.text)\n",
    "#                 blogs_URL[blog_category].append(blog_title_tag.find_element(By.TAG_NAME, 'a').get_attribute(\"href\"))\n",
    "\n",
    "#         page_no = 1\n",
    "\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 remaining_blogs = driver.find_elements(By.CLASS_NAME, 'jeg_pl_md_1')\n",
    "\n",
    "#                 for blog in remaining_blogs:\n",
    "\n",
    "#                     blog_category = blog.find_element(By.CLASS_NAME, 'jeg_post_category').text\n",
    "#                     blog_title_tag = blog.find_element(By.CLASS_NAME, 'jeg_post_title')\n",
    "\n",
    "#                     if blog_category not in blogs_title:\n",
    "#                         blogs_title[blog_category] = []\n",
    "#                         blogs_URL[blog_category] = []\n",
    "#                         blogs_title_set[blog_category] = set()\n",
    "\n",
    "#                     if blog_title_tag.text not in blogs_title_set[blog_category]:\n",
    "\n",
    "#                         blogs_title_set[blog_category].add(blog_title_tag.text)\n",
    "#                         blogs_title[blog_category].append(blog_title_tag.text)\n",
    "#                         blogs_URL[blog_category].append(blog_title_tag.find_element(By.TAG_NAME, 'a').get_attribute(\"href\"))\n",
    "\n",
    "#                 print(f\"Page {page_no} over...\")\n",
    "#                 page_no += 1\n",
    "\n",
    "#                 webdriver.ActionChains(driver).send_keys(Keys.END).perform()\n",
    "\n",
    "#                 next_page_button = driver.find_element(By.XPATH, \"//a[@class='page_nav next']\")\n",
    "\n",
    "#                 webdriver.ActionChains(driver).move_to_element(next_page_button).click(next_page_button).perform()\n",
    "\n",
    "#             except NoSuchElementException:\n",
    "#                 print(f\"All Blogs from https://dataconomy.com/category/{category_sub_URL}/ Registered!\\n\")\n",
    "#                 break\n",
    "\n",
    "# scrape_blogs(driver, category_sub_URLs, blogs_URL, blogs_title, blogs_title_set)\n",
    "\n",
    "# index = 1\n",
    "# for URL_category, urls in blogs_URL.items():\n",
    "#     for url in urls:\n",
    "#         print(index, URL_category, url)\n",
    "#         index += 1\n",
    "\n",
    "# def save_to_text_file(title, link, author_name, author_URL, date, text, summary, folder_name):\n",
    "    \n",
    "#     # Remove invalid characters from the title to create a valid filename for the text file\n",
    "#     filename = os.path.join(folder_name, re.sub(r'[\\/:*?\"<>|]', '', title) + '.txt')\n",
    "\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "#     # Open the file in write mode\n",
    "#     with open(filename, 'w', encoding='utf-8') as file:\n",
    "#         # Write the information to the file\n",
    "#         file.write(f'Title: {title}\\n\\n')\n",
    "#         file.write(f'Article Link: {link}\\n\\n')\n",
    "#         file.write(f'Author: {author_name}\\n\\n')\n",
    "#         file.write(f'Publish Date: {date}\\n\\n')\n",
    "#         file.write(f'Article:\\n{text}\\n\\n')\n",
    "#         file.write(f'Summary: {summary}\\n\\n')\n",
    "#         file.write(f'Other Blogs by {author_name}: {author_URL}\\n')\n",
    "\n",
    "#     print(f'Information saved to {filename}')\n",
    "\n",
    "# for URL_category, url_list in blogs_URL.items():\n",
    "#     for url in url_list:\n",
    "\n",
    "#         time.sleep(5)\n",
    "\n",
    "#         # Make a request to the URL\n",
    "#         response = requests.get(url)\n",
    "#         html = response.text\n",
    "\n",
    "#         # Use BeautifulSoup to parse the HTML\n",
    "#         soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#         # Find author information\n",
    "#         author = soup.find('div', class_='jeg_meta_author').find('a')\n",
    "\n",
    "#         # Create an Article object\n",
    "#         article = Article(url)\n",
    "#         article.set_html(html)\n",
    "#         article.parse()\n",
    "#         article.nlp()\n",
    "\n",
    "#         # Extract information from the article\n",
    "#         title = article.title\n",
    "\n",
    "#         filename = os.path.join(URL_category, re.sub(r'[\\/:*?\"<>|]', '', title) + '.txt')\n",
    "#         # Check if the file already exists\n",
    "#         if os.path.exists(filename):\n",
    "#             print(f'File {filename} already exists. Skipping...')\n",
    "#             continue\n",
    "        \n",
    "#         # Extract information from the article\n",
    "#         link = article.url\n",
    "#         author_name = author.text\n",
    "#         author_URL = author.get('href')\n",
    "#         date = article.publish_date.date()\n",
    "#         text = article.text\n",
    "#         summary = article.summary\n",
    "\n",
    "#         # Save information to a text file in the respective folder\n",
    "#         save_to_text_file(title, link, author_name, author_URL, date, text, summary, URL_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f88be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
