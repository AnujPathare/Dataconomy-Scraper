Title: â€œWrite a ton of code. Donâ€™t watch TV. â€ â€“ An Interview with Data Scientist Erik Bernhardsson

Article Link: https://dataconomy.com/2015/11/09/write-a-ton-of-code-dont-watch-tv-an-interview-with-data-scientist-erik-bernhardsson/

Author: Peadar Coyle

Publish Date: 2015-11-09

Article:
Erik likes to work with smart people and deliver great software. After 5+ years at Spotify, he recently left for new and exciting startup in NYC where he is leading the engineering team.

At Spotify, Erik built up and lead the team responsible for music recommendations and machine learning. They designed and built many large scale machine learning algorithms we use to power the recommendation features: the radio feature, the â€œDiscoverâ€â€‹ page, â€œRelated Artistsâ€â€‹, and much more. He also authored Luigi, which is a workflow manager in Python with 3,000+ stars on Github â€“ used by Foursquare, Quora, Stripe, Asana, and more.

Follow Peadarâ€™s series of interviews with data scientists here.

What project have you worked on do you wish you could go back to, and do better?

Likeâ€¦ everything I ever built. But I think thatâ€™s part of the learning experience. Especially working with real users, you never know whatâ€™s going to happen. Thereâ€™s no clear problem formulation, no clear loss function, lots of various data sets to use. Of course youâ€™re going to waste too much time doing something that turns out to nothing. But research is that way. Learning stuff is what matters and kind of by definition you have to do stupid shit before you learned it. Sorry for a super unclear answer ğŸ™‚

The main thing I did wrong for many years was I built all this cool stuff but never really made it into prototypes that other people could play around with. So I learned something very useful about communication and promoting your ideas.

What advice do you have to younger analytics professionals and in particular PhD students in the Sciences?

Write a ton of code. Donâ€™t watch TV. ğŸ™‚

[bctt tweet=â€Write a ton of code. Donâ€™t watch TV. :)â€]

I really think showcasing cool stuff on Github and helping out other projects is a great way to learn and also to demonstrate market validation of your code.

Seriously, I think everyone can kick ass at almost anything as long as you spend a ridiculous amount of time on it. As long as youâ€™re motivated by something, use that by focusing on something 80% of your time being awake.

I think people generally get motivated by coming up with various proxies for success. So be very careful about choosing the right proxies. I think people in academia often validate themselves in terms of things people in the industry donâ€™t care about and things that doesnâ€™t necessarily correlate with a successful career. Itâ€™s easy to fall down into a rabbit hole and become extremely good at say deep learning (or anything), but at a company that means youâ€™re just some expert that will have a hard time getting impact beyond your field. Looking back on my own situation I should have spent a lot more time figuring out how to get other people excited about my ideas instead of perfecting ML algorithms (maybe similar to last question).

What do you wish you knew earlier about being a data scientist?

I donâ€™t consider myself a data scientist so not sure ğŸ™‚

Thereâ€™s a lot of definitions floating around about what a data scientist does. I have had this theory for a long time but just ran into this blog post the other day. I think it summarizes my own impression pretty well. Thereâ€™s two camps, one is the â€œbusiness insightsâ€ side, one is the â€œproduction ML engineerâ€ side. I managed teams at Spotify on both sides. Itâ€™s very different.

If you want to understand the business and generate actionable insights, then in my experience you need pretty much no knowledge of statistics and machine learning. It seems like people think with ML you can generate these super interesting insights about a business but in my experience itâ€™s very rare. Sometimes we had people coming in writing a masterâ€™s thesis about churn prediction and you can get a really high AUC but itâ€™s almost impossible to use that model for anything. So it really just boils down to doing lots of highly informed A/B tests. And above all, having deep empathy for user behavior. What I mean is you really need to understand how your users think in order to generate hypotheses to test.

For the other camp, in my experience understanding backend development is super important. Iâ€™ve seen companies where thereâ€™s a â€œML research teamâ€ and a â€œimplementation teamâ€ and thereâ€™s a â€œthrow it over the fenceâ€ attitude, but it doesnâ€™t work. Iteration cycles get 100x larger and incentives just get misaligned. So I think for anyone who wants to build cool ML algos, they should also learn backend and data engineering.

How do you respond when you hear the phrase â€˜big dataâ€™?

Love it. Seriously, thereâ€™s this weird anti-trend of people bashing big data. I throw up every time I see another tweet like â€œYou can get a machine with 1TB of ram for $xyz. You donâ€™t have big dataâ€. I almost definitely had big data at Spotify. We trained models with 10B parameters on 10TB data sets all the time. There is a lot of those problems in the industry for sure. Unfortunately sampling doesnâ€™t always work.

The other thing I think those people get wrong is the production aspect of it. Things like Hadoop forces your computation into fungible units that means you donâ€™t have to worry about computers breaking down. It might be 10x slower than if you had specialized hardware, but thatâ€™s fine because you can have 100 teams running 10000 daily jobs and things rarely crash â€“ especially if you use Luigi. ğŸ™‚

But Iâ€™m sure thereâ€™s a fair amount of snake oil Hadoop consultants who convince innocent teams they need it.

The other part of â€œbig dataâ€ is that itâ€™s at the far right of the hype cycle. Have you been to a Hadoop conference? Itâ€™s full of people in oversized suits talking about compliance now. At some point weâ€™ll see deep learning or flux architecture or whatever going down the same route.

How do you go about framing a data problem â€“ in particular, how do you avoid spending too long, how do you manage expectations etc. How do you know what is good enough?

Ideally you can iterate on it with real users and see what the impact is. If not, you need to introduce some proxy metrics. Thatâ€™s a whole art form in itself.

Itâ€™s good enough when the opportunity cost outweighs the benefit ğŸ™‚ I.e. the marginal return of time invested is lower than for something else. I think itâ€™s good to keep a backlog full of 100s of ideas so that you can prioritize based on expected ROI at any time. I donâ€™t know if thatâ€™s a helpful answer but prioritization is probably the hardest problem to solve and it really just boils down to having some rules of thumb.

(image credit: Jer Thorp, CC2.0)

Summary: :)â€]I really think showcasing cool stuff on Github and helping out other projects is a great way to learn and also to demonstrate market validation of your code.
I donâ€™t consider myself a data scientist so not sure ğŸ™‚Thereâ€™s a lot of definitions floating around about what a data scientist does.
How do you respond when you hear the phrase â€˜big dataâ€™?
You donâ€™t have big dataâ€.
The other part of â€œbig dataâ€ is that itâ€™s at the far right of the hype cycle.

Other Blogs by Peadar Coyle: https://dataconomy.com/author/pcoyle/
